{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGWlx78AFmrQ"
   },
   "source": [
    "# Product Recognition on Store Shelves\n",
    "## Computer Vision and Image Processing - Exam Project\n",
    "### Nicholas Antonio Carroll, nicholas.carroll@studio.unibo.it - Laura Mazzuca, laura.mazzuca@studio.unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lNc-pyIFmrU"
   },
   "source": [
    "## Step A: Multiple Product Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLP9ETu_FmrV"
   },
   "source": [
    "### Local Invariant Features Paradigm\n",
    "\n",
    "Allows to successfully identify objects in scene from a \n",
    "single model image per object.\n",
    "\n",
    "**Four steps**:\n",
    "1. **Detection**: Identify salient repeatable points (Keypoints) in model and scene images.\n",
    "2. **Description**: Create a unique description of each point, usually based on its local pixel neighborhood.\n",
    "3. **Matching**: Match point from scene and model according to a similarity function between the descriptors.\n",
    "4. **Position Estimation**: Estimate the position of the object in the scene image given enough matching points "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9blb2mmisDnP"
   },
   "source": [
    "### Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20472,
     "status": "ok",
     "timestamp": 1638814884324,
     "user": {
      "displayName": "Nicholas Carroll",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16713859934441982955"
     },
     "user_tz": -60
    },
    "id": "a_f2sO0WVVAe",
    "outputId": "cd5bcff0-6c7e-4e9e-b298-83721b588b8f"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  #Connect to google drive folder, needed only if running notebook on colab\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VF8bX6-PVdhU"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install opencv-contrib-python==4.3.0.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1638814933745,
     "user": {
      "displayName": "Nicholas Carroll",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16713859934441982955"
     },
     "user_tz": -60
    },
    "id": "knwqBTWMFmrX",
    "outputId": "b17cae8a-5eff-4bdb-ecbc-e66f2f795fd2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "if IN_COLAB:\n",
    "  scenes_location = 'gdrive/MyDrive/shelf-product-recognition/images/scenes/'\n",
    "  models_location = 'gdrive/MyDrive/shelf-product-recognition/images/models/'\n",
    "else:\n",
    "  scenes_location = 'images/scenes/'\n",
    "  models_location = 'images/models/'\n",
    "\n",
    "scenes=['m1.png','m2.png','m3.png', 'm4.png','m5.png']\n",
    "models=['0.jpg','1.jpg','11.jpg','19.jpg','24.jpg','25.jpg','26.jpg']\n",
    "\n",
    "for i in range(len(scenes)):\n",
    "  scenes[i] = scenes_location+scenes[i]\n",
    "\n",
    "for i in range(len(models)):\n",
    "  models[i] = models_location+models[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boYU1bw2FmrZ"
   },
   "source": [
    "Load the train images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fBUMrNlSFmrZ"
   },
   "outputs": [],
   "source": [
    "img_train = []\n",
    "for p in scenes:\n",
    "    img_train.append(cv2.imread(p, cv2.IMREAD_GRAYSCALE)) # trainImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhQSINUSFmra"
   },
   "source": [
    "And all the query images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0vIuHhbwFmra"
   },
   "outputs": [],
   "source": [
    "img_query = []\n",
    "i=0\n",
    "max_h = 0\n",
    "resize = True\n",
    "for p in models:\n",
    "    img_query.append(cv2.imread(p, cv2.IMREAD_GRAYSCALE)) # queryImage\n",
    "    if resize and max_h < img_query[i].shape[0]:\n",
    "        max_h = img_query[i].shape[0]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JvQsHtYSbFCi"
   },
   "outputs": [],
   "source": [
    "if resize:\n",
    "    i=0\n",
    "    for img in img_query:\n",
    "        if img.shape[0] < max_h:\n",
    "            height = int(max_h)\n",
    "            width = int(img.shape[1]*max_h/img.shape[0])\n",
    "            dim = (width, height)\n",
    "            # resize image\n",
    "            img_query[i] = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7wmKw3UFmrb"
   },
   "source": [
    "### 1. Keypoint Detection\n",
    "Initialize the SIFT detector object in the following way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nEBRd7-5Fmrc"
   },
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "if IN_COLAB:\n",
    "  sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=5, contrastThreshold=0.11, sigma=1.4) #very good results with both resized and not resized images\n",
    "else:\n",
    "  sift = cv2.SIFT_create(nOctaveLayers=5, contrastThreshold=0.11, sigma=1.4) #very good results with both resized and not resized images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq0aMlnHFmrc"
   },
   "source": [
    "Then we need to find keypoints. We will use _detect_ method of the SIFT detector object. Let us find the keypoints of all the query images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8hGoPm3AFmrd"
   },
   "outputs": [],
   "source": [
    "# find the keypoints and descriptors with SIFT\n",
    "kp_query = []\n",
    "for img in img_query:\n",
    "    kp_query.append(sift.detect(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pvpW5OwFmrd"
   },
   "source": [
    "Now, let us find and draw the keypoints for the train images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n4VbBZVeFmre"
   },
   "outputs": [],
   "source": [
    "kp_train = []\n",
    "for img in img_train:\n",
    "    kp_train.append(sift.detect(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-Y8U1HjFmre"
   },
   "source": [
    "### 2. Keypoints Description \n",
    "\n",
    "Compute for each keypoint a unique description usually based on the nearby pixels (descriptor support).\n",
    "We will use the SIFT descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ngCP0yoHFmrf"
   },
   "outputs": [],
   "source": [
    "# Describing keypoints for query and train images\n",
    "des_query = np.empty(len(img_query), dtype=object)\n",
    "des_train = np.empty(len(img_train), dtype=object)\n",
    "\n",
    "for i in range(len(img_query)):\n",
    "    kp_query[i], des_query[i] = sift.compute(img_query[i], kp_query[i])\n",
    "for i in range(len(img_train)):\n",
    "    kp_train[i], des_train[i] = sift.compute(img_train[i], kp_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCx_g9IjFmrf"
   },
   "source": [
    "### 3. Feature Matching\n",
    "\n",
    "We will use an approximate kd-tree algorithm from **FLANN** (Fast Library for Approximate Nearest Neighbors) included in OpenCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nhSuJbgyFmrg"
   },
   "outputs": [],
   "source": [
    "# Defining index for approximate kdtree algorithm\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "\n",
    "# Defining parameters for algorithm \n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "\n",
    "# Defining search params.\n",
    "# checks=50 specifies the number of times the trees in the index should be recursively traversed.\n",
    "# Higher values gives better precision, but also takes more time\n",
    "search_params = dict(checks = 500)\n",
    "\n",
    "# Initializing matcher\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Matching and finding the 2 closest elements for each query descriptor.\n",
    "matches_per_image = np.empty(len(img_train), dtype=object)\n",
    "i = 0 #index for the train images\n",
    "\n",
    "\n",
    "for des_t in des_train: #for each scene\n",
    "    matches_per_image[i] = [] #create a list that will hold the matches wrt each model image\n",
    "    for des_q in des_query: #for each query image\n",
    "\n",
    "        matches_per_image[i].append(flann.knnMatch(des_q,des_t,k=2)) #compute the matches\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mFHAQcwaFmrg"
   },
   "outputs": [],
   "source": [
    "good = np.empty(len(img_train), dtype=object)\n",
    "i=0\n",
    "j=0\n",
    "for mat_img in matches_per_image:#for each list of matches related to an image wrt a train image\n",
    "    good[i] = np.empty(len(img_query), dtype=object)\n",
    "    j=0\n",
    "    for matches in mat_img: # for each list of matches related to the train image j\n",
    "        good[i][j] = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.7*n.distance:\n",
    "                good[i][j].append(m)\n",
    "        j=j+1\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30uvVWMoFmrg"
   },
   "source": [
    "### 4. Position Estimation\n",
    "We can use  **Random Sample Consensus (RANSAC)**, an algorithm to fit a parametric model to noisy data. In our case estimate an homography from good matches while identifying and discarding the wrong ones.\n",
    "\n",
    "Since images can have different thresholds to be recognized in a scene, the check_overlap funcion computes the centroid of the bounding box points computed thanks to the homography and if they appear to be overlapping, the one with the most good points gets added to the dst vector, while the other gets removed from it. Then, when a whole scene is scanned, the bounding boxes are finally drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aeTT7cZ4bFCm"
   },
   "outputs": [],
   "source": [
    "def check_overlap(dst, j):\n",
    "    if j == 0:\n",
    "        return -1\n",
    "    \n",
    "    control_centroid = dst[j].mean(axis=0)\n",
    "    print('examining image #{} with centroid = {}'.format(j,control_centroid))\n",
    "    \n",
    "    for i in range(j):\n",
    "        if dst[i] is not None:\n",
    "            centroid = dst[i].mean(axis=0)\n",
    "            print('confronting with image #{} with centroid = {}'.format(i,centroid))\n",
    "            \n",
    "            difference = np.absolute(centroid-control_centroid)\n",
    "            if difference[0][0] <= 15.0 and difference[0][1] <=15.0:\n",
    "                return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1638434618829,
     "user": {
      "displayName": "Nicholas Carroll",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16713859934441982955"
     },
     "user_tz": -60
    },
    "id": "HnA6EFOKFmrh",
    "outputId": "55dfe53f-6186-4db5-dae8-63593baf5535"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scene_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7996/2964134423.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Checking if we found enough matching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg_train_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscene_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimg_train_rgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# trainImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scene_paths' is not defined"
     ]
    }
   ],
   "source": [
    "# Checking if we found enough matching\n",
    "img_train_rgb = []\n",
    "for p in scene_paths:\n",
    "    img_train_rgb.append(cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)) # trainImage\n",
    "\n",
    "if resize:\n",
    "    MIN_MATCH_COUNT = 116 \n",
    "else:\n",
    "    MIN_MATCH_COUNT = 80\n",
    "    \n",
    "matchesMask = np.empty(len(img_train), dtype=object)\n",
    "dst = np.empty(len(img_train), dtype=object)\n",
    "color = (0, 255, 0)\n",
    "\n",
    "for i in range(len(img_train)):\n",
    "    cur_kp_train = kp_train[i]\n",
    "    matchesMask[i] = np.empty(len(img_query), dtype=object)\n",
    "    dst[i] = np.empty(len(img_query), dtype=object)\n",
    "    for j in range(len(img_query)):\n",
    "        if len(good[i][j])>MIN_MATCH_COUNT:\n",
    "            cur_kp_query = kp_query[j]\n",
    "            # building the corrspondences arrays of good matches\n",
    "            src_pts = np.float32([ cur_kp_query[m.queryIdx].pt for m in good[i][j] ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ cur_kp_train[m.trainIdx].pt for m in good[i][j] ]).reshape(-1,1,2)\n",
    "            # Using RANSAC to estimate a robust homography. \n",
    "            # It returns the homography M and a mask for the discarded points\n",
    "            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "            # Mask of discarded point used in visualization\n",
    "            matchesMask[i][j] = mask.ravel().tolist()\n",
    "            # Corners of the query image\n",
    "            h,w = img_query[j].shape\n",
    "            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "\n",
    "            # Projecting the corners into the train image\n",
    "            dst[i][j] = cv2.perspectiveTransform(pts,M)\n",
    "            \n",
    "            overlap_idx = check_overlap(dst[i], j)\n",
    "            if overlap_idx > -1:\n",
    "                if len(good[i][j]) >= len(good[i][overlap_idx]):\n",
    "                    dst[i][overlap_idx] = None\n",
    "                else:\n",
    "                    dst[i][j] = None         \n",
    "        else:\n",
    "            print( \"Not enough matches are found - {}/{}\".format(len(good[i][j]), MIN_MATCH_COUNT) )\n",
    "            matchesMask[i][j] = None\n",
    "            \n",
    "    for j in range(len(img_query)):\n",
    "        if dst[i][j] is not None:\n",
    "            print('drawing on scene #{} bounding box #{}'.format(i,j))\n",
    "            # Drawing the bounding box\n",
    "            img_train_rgb[i] = cv2.polylines(img_train_rgb[i],[np.int32(dst[i][j])],True,color,3, cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwFv8XoeFmrj"
   },
   "source": [
    "Finally, if we want to draw the matches we can do it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2383,
     "status": "ok",
     "timestamp": 1638434627265,
     "user": {
      "displayName": "Nicholas Carroll",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16713859934441982955"
     },
     "user_tz": -60
    },
    "id": "ZnieFGLYFmrj",
    "outputId": "f73090bd-d3e7-42dc-bcec-6bd369ea6744"
   },
   "outputs": [],
   "source": [
    "for img in img_train_rgb:\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5574,
     "status": "ok",
     "timestamp": 1638434638183,
     "user": {
      "displayName": "Nicholas Carroll",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16713859934441982955"
     },
     "user_tz": -60
    },
    "id": "GXm3d1bfFmrj",
    "outputId": "80d3179b-fdcd-45f7-8a6a-735d121a2958"
   },
   "outputs": [],
   "source": [
    "# Drawing the matches\n",
    "for i in range(len(img_train)):\n",
    "    for j in range(len(img_query)):\n",
    "        if dst[i][j] is not None:\n",
    "            draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                   singlePointColor = None, # not draw keypoints only matching lines\n",
    "                   matchesMask = matchesMask[i][j], # draw only inliers\n",
    "                   flags = 2) # not draw keypoints only lines\n",
    "            img3 = cv2.drawMatches(img_query[j],kp_query[j],img_train[i],kp_train[i],good[i][j],None,**draw_params)\n",
    "            plt.imshow(img3, 'gray')\n",
    "            plt.show()\n",
    "            print(len(good[i][j]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "shelf-product-recognition-step-A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
