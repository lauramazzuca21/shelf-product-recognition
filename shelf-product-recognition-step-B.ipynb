{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"shelf-product-recognition-step-B.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"u09a6m2WbeqB"},"source":["# Product Recognition on Store Shelves\n","## Computer Vision and Image Processing - Exam Project\n","### Nicholas Antonio Carroll, nicholas.carroll@studio.unibo.it - Laura Mazzuca, laura.mazzuca@studio.unibo.it"]},{"cell_type":"markdown","metadata":{"id":"9nKRPV1WbeqF"},"source":["## Step B: Multiple Instance Detection\n"]},{"cell_type":"markdown","metadata":{"id":"7kbXp8kybeqG"},"source":["### Local Invariant Features with General Hough Transform Star Model\n","\n","###### Offline phase\n","1. **Keypoint Detection**: Identify salient repeatable points (Keypoints) in model and scene images (with SIFT).\n","2. **Compute Descriptor**: \n","    - compute barycentre **Pc** of all keypoints\n","    - compute voting parameter **Vi** for each keypoint **Ki** with position **Pi** as ***Vi = Pc -Pi***\n","\n","###### Online phase\n","3. **GHT**: Match point from scene and model according to joining vectors detection.\n","4. **Position Estimation**: Estimate the position of the object in the scene image given enough matching points "]},{"cell_type":"markdown","metadata":{"id":"ufww-4SLbud8"},"source":["## Environment setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ArYPLZZbxkc","executionInfo":{"status":"ok","timestamp":1638814736752,"user_tz":-60,"elapsed":316,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}},"outputId":"cfb49f37-b516-44a1-9027-0b817ae6dec6"},"source":["import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","if IN_COLAB:\n","  #Connect to google drive folder, needed only if running notebook on colab\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"NUmwFgrGZrhr"},"source":["If working in colab install the correct version of opencv"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ou0tXU7bzeh","executionInfo":{"status":"ok","timestamp":1638814739731,"user_tz":-60,"elapsed":2664,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}},"outputId":"37ca8cac-6279-4c8e-95c2-b361d0490a72"},"source":["if IN_COLAB:\n","  !pip install opencv-contrib-python==4.3.0.36"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-contrib-python==4.3.0.36 in /usr/local/lib/python3.7/dist-packages (4.3.0.36)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==4.3.0.36) (1.19.5)\n"]}]},{"cell_type":"code","metadata":{"id":"LURu183wbeqI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638814846833,"user_tz":-60,"elapsed":319,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}},"outputId":"cfc86f2c-90ec-4c88-e843-d4d0d37dea3e"},"source":["import numpy as np\n","import cv2\n","from matplotlib import pyplot as plt\n","\n","if IN_COLAB:\n","  scenes_location = 'gdrive/MyDrive/shelf-product-recognition/images/scenes/'\n","  models_location = 'gdrive/MyDrive/shelf-product-recognition/images/models/'\n","else:\n","  scenes_location = 'images/scenes/'\n","  models_location = 'images/models/'\n","\n","scenes=['m1.png','m2.png','m3.png', 'm4.png','m5.png']\n","models=['0.jpg','1.jpg','11.jpg','19.jpg','24.jpg','25.jpg','26.jpg']\n","\n","for i in range(len(scenes)):\n","  scenes[i] = scenes_location+scenes[i]\n","\n","for i in range(len(models)):\n","  models[i] = models_location+models[i]\n","\n","print(scenes)\n","print(models)"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["['gdrive/MyDrive/shelf-product-recognition/images/scenes/m1.png', 'gdrive/MyDrive/shelf-product-recognition/images/scenes/m2.png', 'gdrive/MyDrive/shelf-product-recognition/images/scenes/m3.png', 'gdrive/MyDrive/shelf-product-recognition/images/scenes/m4.png', 'gdrive/MyDrive/shelf-product-recognition/images/scenes/m5.png']\n","['gdrive/MyDrive/shelf-product-recognition/images/models/0.jpg', 'gdrive/MyDrive/shelf-product-recognition/images/models/1.jpg', 'gdrive/MyDrive/shelf-product-recognition/images/models/11.jpg', 'gdrive/MyDrive/shelf-product-recognition/images/models/19.jpg', 'gdrive/MyDrive/shelf-product-recognition/images/models/24.jpg', 'gdrive/MyDrive/shelf-product-recognition/images/models/25.jpg', 'gdrive/MyDrive/shelf-product-recognition/images/models/26.jpg']\n"]}]},{"cell_type":"markdown","metadata":{"id":"PebsGBxsbeqM"},"source":["All the query images:"]},{"cell_type":"code","metadata":{"id":"lK2oU3mRbeqN","executionInfo":{"status":"ok","timestamp":1638814745136,"user_tz":-60,"elapsed":5407,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["img_query = []\n","i=0\n","max_h = 0\n","resize = True\n","for p in model_paths:\n","    img_query.append(cv2.imread(p, cv2.IMREAD_GRAYSCALE)) # queryImage\n","    if resize and max_h < img_query[i].shape[0]:\n","        max_h = img_query[i].shape[0]\n","    i=i+1"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8U4SQ7nY98b","executionInfo":{"status":"ok","timestamp":1638814745136,"user_tz":-60,"elapsed":5,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["if resize:\n","    i=0\n","    for img in img_query:\n","        if img.shape[0] < max_h:\n","            height = int(max_h)\n","            width = int(img.shape[1]*max_h/img.shape[0])\n","            dim = (width, height)\n","            # resize image\n","            img_query[i] = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n","        i=i+1"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rPpKFZztY98b"},"source":["## Offline Phase"]},{"cell_type":"markdown","metadata":{"id":"IKgpTdPJbeqP"},"source":["### 1. Keypoint Detection\n","Initialize the SIFT detector object in the following way:\n"]},{"cell_type":"code","metadata":{"id":"zI6AS3xKbeqQ","executionInfo":{"status":"ok","timestamp":1638814745137,"user_tz":-60,"elapsed":5,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["# Initiate SIFT detector\n","if IN_COLAB:\n","  sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=5, contrastThreshold=0.11, sigma=1.4) #very good results with both resized and not resized images\n","else:\n","  sift = cv2.SIFT_create(nOctaveLayers=5, contrastThreshold=0.11, sigma=1.4) #very good results with both resized and not resized images\n"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Jen4lesbeqR"},"source":["Then we need to find keypoints. We will use _detect_ method of the SIFT detector object. Let us find the keypoints of all the query images:"]},{"cell_type":"code","metadata":{"id":"fxCDy2HvbeqR","executionInfo":{"status":"ok","timestamp":1638814747074,"user_tz":-60,"elapsed":1942,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["# find the keypoints and descriptors with SIFT\n","kp_query = []\n","for img in img_query:\n","    kp_query.append(sift.detect(img))"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsrYcpzzbeqU"},"source":["### 2. Keypoints Description \n","\n","Compute for each keypoint a unique description usually based on the nearby pixels (descriptor support). We will use the SIFT descriptor."]},{"cell_type":"code","metadata":{"id":"Bi3s2OQofdQO","executionInfo":{"status":"ok","timestamp":1638814747075,"user_tz":-60,"elapsed":7,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["#Calculates the barycenter of the keypoints of an image\n","def calculateBarycenter(keyPoints):\n","  x=0\n","  y=0\n","  for kp in keyPoints:\n","    x += kp.pt[0]\n","    y += kp.pt[1]\n","  x = x/len(keyPoints)\n","  y = y/len(keyPoints)\n","\n","  return (x,y)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccGJdGSJdrxh","executionInfo":{"status":"ok","timestamp":1638814747076,"user_tz":-60,"elapsed":7,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["#The extended keypoint class contains the coordinates of the point, angle, size, descriptor and the voting parameter\n","class ExtendedKeyPoint:\n","  def __init__(self, kp, pc, desc):\n","    self.kp = kp\n","    self.desc = desc\n","    #distance between barycenter and the point\n","    self.vi = np.sqrt(np.power(pc[0] - kp.pt[0], 2) + np.power(pc[1] - kp.pt[1], 2))\n"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"xrG_I8WY5LvR","executionInfo":{"status":"ok","timestamp":1638814748672,"user_tz":-60,"elapsed":1601,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["# Describing keypoints for query and train images\n","des_query = np.empty(len(img_query), dtype=object)\n","\n","for i in range(len(img_query)):\n","    kp_query[i], des_query[i] = sift.compute(img_query[i], kp_query[i])"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5ZmsslPhdB2","executionInfo":{"status":"ok","timestamp":1638814748672,"user_tz":-60,"elapsed":4,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["#Calculating extended Keypoints\n","kp_query_extended = []\n","for i in range(len(kp_query)):\n","  pc = calculateBarycenter(kp_query[i])\n","  kp_query_extended.append([])\n","\n","  for k in range(len(kp_query[i])):\n","    kp_query_extended[i].append(ExtendedKeyPoint(kp_query[i][k], pc, des_query[i][k]))"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ipFSrc43Y98e"},"source":["## Online Phase"]},{"cell_type":"markdown","metadata":{"id":"e-r3pXWIbeqL"},"source":["Load the train images:"]},{"cell_type":"code","metadata":{"id":"EIlkug0xbeqM","executionInfo":{"status":"ok","timestamp":1638814752383,"user_tz":-60,"elapsed":3714,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["img_train = []\n","for p in scene_paths:\n","    img_train.append(cv2.imread(p, cv2.IMREAD_GRAYSCALE)) # trainImage"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LhNBjz4wbeqS"},"source":["Let us find the keypoints for the train images:"]},{"cell_type":"code","metadata":{"id":"M_3Q2PuSbeqS","executionInfo":{"status":"ok","timestamp":1638814753336,"user_tz":-60,"elapsed":957,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["kp_train = []\n","for img in img_train:\n","    kp_train.append(sift.detect(img))"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3dFJ-HBbeqW"},"source":["### 3. Generalized Hough Transform"]},{"cell_type":"code","metadata":{"id":"JTkSf5ZNY98f"},"source":["def ght():\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kZ2-M_YbeqW","executionInfo":{"status":"aborted","timestamp":1638814753780,"user_tz":-60,"elapsed":453,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["# Defining index for approximate kdtree algorithm\n","FLANN_INDEX_KDTREE = 1\n","\n","# Defining parameters for algorithm \n","index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n","\n","# Defining search params.\n","# checks=50 specifies the number of times the trees in the index should be recursively traversed.\n","# Higher values gives better precision, but also takes more time\n","search_params = dict(checks = 500)\n","\n","# Initializing matcher\n","flann = cv2.FlannBasedMatcher(index_params, search_params)\n","\n","# Matching and finding the 2 closest elements for each query descriptor.\n","matches_per_image = np.empty(len(img_train), dtype=object)\n","i = 0 #index for the train images\n","\n","for des_t in des_train: #for each scene\n","    matches_per_image[i] = [] #create a list that will hold the matches wrt each model image\n","    for des_q in des_query: #for each query image\n","        matches_per_image[i].append(flann.knnMatch(des_q,des_t,k=2)) #compute the matches\n","    i = i+1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5g6bFoLIbeqX","executionInfo":{"status":"aborted","timestamp":1638814753783,"user_tz":-60,"elapsed":456,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["good = np.empty(len(img_train), dtype=object)\n","i=0\n","j=0\n","for mat_img in matches_per_image:#for each list of matches related to an image wrt a train image\n","    good[i] = np.empty(len(img_query), dtype=object)\n","    j=0\n","    for matches in mat_img: # for each list of matches related to the train image j\n","        good[i][j] = []\n","        for m,n in matches:\n","            if m.distance < 0.7*n.distance:\n","                good[i][j].append(m)\n","        j=j+1\n","    i=i+1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGhrQl1JbeqY"},"source":["### 4. Position Estimation\n","We can use  **Random Sample Consensus (RANSAC)**, an algorithm to fit a parametric model to noisy data. In our case estimate an homography from good matches while identifying and discarding the wrong ones.\n","\n","### RANSAC algorithm:\n","Given a set of observation $O=\\{o_1...o_n\\}$ and a certain parametric model $M$, repeat iteratively:\n","1. Pick a random (small) subset $I$ of $O$ called inlier set.\n","2. Fit a model $M_i$ according to the observations in $I$.\n","3. Test all the other observations against $M_i$, add to a new set $C$ (consensus set) all the observations that fit $M_i$ according to a model specific loss function.\n","4. If the consensus set is bigger than the one associated with the current best model $M_b$, proceed to step 5, other way return to step 1.\n","5. Re-compute $M_i$ according to the observations in $I \\cup C$, then set $M_b=M_i$. Restart from step 1. \n","\n","The procedure is repeated for a fixed amount of steps, at the end the best model is returned.\n","\n","A simple example of RANSAC result used to fit a line to a set of 2D point with Euclidean distance as loss function:\n","In our case we use the homography as parametric model and the reprojection error as loss function.\n","\n","If we compute a bounding box transforming the corner of the model image in the scene image reference system with an Homography computed using RANSAC we obtain the following result:"]},{"cell_type":"code","metadata":{"id":"mONmgrLRY98g","executionInfo":{"status":"aborted","timestamp":1638814753784,"user_tz":-60,"elapsed":457,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["def check_overlap(dst, j):\n","    if j == 0:\n","        return -1\n","    \n","    control_centroid = dst[j].mean(axis=0)\n","    print('examining image #{} with centroid = {}'.format(j,control_centroid))\n","    \n","    for i in range(j):\n","        if dst[i] is not None:\n","            centroid = dst[i].mean(axis=0)\n","            print('confronting with image #{} with centroid = {}'.format(i,centroid))\n","            \n","            difference = np.absolute(centroid-control_centroid)\n","            if difference[0][0] <= 15.0 and difference[0][1] <=15.0:\n","                return i\n","    return -1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"foqFuXbnbeqY","executionInfo":{"status":"aborted","timestamp":1638814753784,"user_tz":-60,"elapsed":456,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["# Checking if we found enough matching\n","img_train_rgb = []\n","for p in scene_paths:\n","    img_train_rgb.append(cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)) # trainImage\n","\n","if resize:\n","    MIN_MATCH_COUNT = 116 \n","else:\n","    MIN_MATCH_COUNT = 80\n","    \n","matchesMask = np.empty(len(img_train), dtype=object)\n","dst = np.empty(len(img_train), dtype=object)\n","color = (0, 255, 0)\n","\n","for i in range(len(img_train)):\n","    cur_kp_train = kp_train[i]\n","    matchesMask[i] = np.empty(len(img_query), dtype=object)\n","    dst[i] = np.empty(len(img_query), dtype=object)\n","    for j in range(len(img_query)):\n","        if len(good[i][j])>MIN_MATCH_COUNT:\n","            cur_kp_query = kp_query[j]\n","            # building the corrspondences arrays of good matches\n","            src_pts = np.float32([ cur_kp_query[m.queryIdx].pt for m in good[i][j] ]).reshape(-1,1,2)\n","            dst_pts = np.float32([ cur_kp_train[m.trainIdx].pt for m in good[i][j] ]).reshape(-1,1,2)\n","            # Using RANSAC to estimate a robust homography. \n","            # It returns the homography M and a mask for the discarded points\n","            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n","\n","            # Mask of discarded point used in visualization\n","            matchesMask[i][j] = mask.ravel().tolist()\n","            # Corners of the query image\n","            h,w = img_query[j].shape\n","            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n","\n","            # Projecting the corners into the train image\n","            dst[i][j] = cv2.perspectiveTransform(pts,M)\n","            \n","            overlap_idx = check_overlap(dst[i], j)\n","            if overlap_idx > -1:\n","                if len(good[i][j]) >= len(good[i][overlap_idx]):\n","                    dst[i][overlap_idx] = None\n","                else:\n","                    dst[i][j] = None         \n","        else:\n","            print( \"Not enough matches are found - {}/{}\".format(len(good[i][j]), MIN_MATCH_COUNT) )\n","            matchesMask[i][j] = None\n","            \n","    for j in range(len(img_query)):\n","        if dst[i][j] is not None:\n","            print('drawing on scene #{} bounding box #{}'.format(i,j))\n","            # Drawing the bounding box\n","            img_train_rgb[i] = cv2.polylines(img_train_rgb[i],[np.int32(dst[i][j])],True,color,3, cv2.LINE_AA)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PvL4naecbeqa"},"source":["Finally, if we want to draw the matches we can do it in the following way:"]},{"cell_type":"code","metadata":{"id":"YawovMBdbeqa","executionInfo":{"status":"aborted","timestamp":1638814753785,"user_tz":-60,"elapsed":457,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["for img in img_train_rgb:\n","    plt.imshow(img)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTXWtmFebeqb","executionInfo":{"status":"aborted","timestamp":1638814753786,"user_tz":-60,"elapsed":457,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":["# Drawing the matches\n","for i in range(len(img_train)):\n","    for j in range(len(img_query)):\n","        if len(good[i][j])>MIN_MATCH_COUNT:\n","            draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n","                   singlePointColor = None, # not draw keypoints only matching lines\n","                   matchesMask = matchesMask[i][j], # draw only inliers\n","                   flags = 2) # not draw keypoints only lines\n","            img3 = cv2.drawMatches(img_query[j],kp_query[j],img_train[i],kp_train[i],good[i][j],None,**draw_params)\n","            plt.imshow(img3, 'gray')\n","            plt.show()\n","            print(len(good[i][j]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkGEteArbeqb","executionInfo":{"status":"aborted","timestamp":1638814753786,"user_tz":-60,"elapsed":457,"user":{"displayName":"Nicholas Carroll","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713859934441982955"}}},"source":[""],"execution_count":null,"outputs":[]}]}